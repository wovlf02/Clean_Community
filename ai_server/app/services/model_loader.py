"""
ëª¨ë¸ ë¡œë”© ì„œë¹„ìŠ¤

í•™ìŠµëœ PyTorch ëª¨ë¸ íŒŒì¼(.pt)ì„ ë¡œë“œí•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.
"""
import os
import torch
from typing import Dict, Tuple
from transformers import AutoTokenizer

from ..models.classifier import MultiLabelClassifier
from ..config import settings
from ..utils.constants import LABELS

# ì „ì—­ ë³€ìˆ˜ë¡œ ë¡œë“œëœ ëª¨ë¸ ì €ìž¥
_models: Dict[str, MultiLabelClassifier] = {}
_tokenizers: Dict[str, AutoTokenizer] = {}
_device: torch.device = None


def get_device() -> torch.device:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜ (GPU ìš°ì„ )

    Returns:
        torch.device: CUDA ì‚¬ìš© ê°€ëŠ¥ì‹œ cuda, ì•„ë‹ˆë©´ cpu
    """
    global _device
    if _device is None:
        _device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    return _device


def load_single_model(
    model_name: str,
    checkpoint_path: str,
    hf_model_name: str
) -> Tuple[MultiLabelClassifier, AutoTokenizer]:
    """
    ë‹¨ì¼ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    ì¶”ë¡  ì „ìš©: .pt íŒŒì¼ì—ì„œ ì „ì²´ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê³ ,
    í† í¬ë‚˜ì´ì €ë§Œ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        model_name: ëª¨ë¸ ì‹ë³„ìž (kcelectra, soongsil, roberta)
        checkpoint_path: .pt íŒŒì¼ ê²½ë¡œ
        hf_model_name: HuggingFace ëª¨ë¸ëª… (í† í¬ë‚˜ì´ì €ìš©)

    Returns:
        (model, tokenizer) íŠœí”Œ
    """
    device = get_device()

    print(f"ðŸ“¦ {model_name} ë¡œë”© ì¤‘... (íŒŒì¼: {checkpoint_path})")

    # 1. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    # 2. state_dict ì¶”ì¶œ
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
        best_f1 = checkpoint.get('best_f1', 'N/A')
    elif isinstance(checkpoint, dict) and 'encoder.embeddings.word_embeddings.weight' in checkpoint:
        state_dict = checkpoint
        best_f1 = 'N/A'
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì²´í¬í¬ì¸íŠ¸ í˜•ì‹: {type(checkpoint)}")

    # 3. state_dictì—ì„œ encoder config ì¶”ì¶œ (hidden_size í™•ì¸)
    # ëŒ€ë¶€ë¶„ì˜ BERT ê³„ì—´ ëª¨ë¸ì€ 768 hidden_size ì‚¬ìš©
    from transformers import AutoConfig

    # í† í¬ë‚˜ì´ì €ë§Œ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(hf_model_name)

    # Configë„ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ (ëª¨ë¸ êµ¬ì¡° ì •ë³´ë§Œ í•„ìš”)
    config = AutoConfig.from_pretrained(hf_model_name)

    # 4. ëª¨ë¸ êµ¬ì¡° ìƒì„± (ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ì—†ì´ configë§Œ ì‚¬ìš©)
    # ì¤‘ìš”: ai-model/src/model.pyì™€ ë™ì¼í•œ êµ¬ì¡°ì—¬ì•¼ í•¨
    model = MultiLabelClassifier(
        model_name=hf_model_name,
        num_labels=len(LABELS),
        dropout_rate=0.3,
        config=config  # config ì „ë‹¬í•˜ì—¬ from_config ì‚¬ìš©
    )

    # 5. ì €ìž¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    # strict=True: ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¤ë¥´ë©´ ì—ëŸ¬ ë°œìƒ (ë¬¸ì œ ì¡°ê¸° ë°œê²¬)
    try:
        model.load_state_dict(state_dict, strict=True)
        print(f"âœ… {model_name} ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ (strict=True)")
    except RuntimeError as e:
        print(f"âš ï¸ strict=True ë¡œë“œ ì‹¤íŒ¨, strict=Falseë¡œ ìž¬ì‹œë„: {e}")
        # í˜¸í™˜ì„± ë¬¸ì œ ì‹œ partial ë¡œë”© ì‹œë„
        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        if missing:
            print(f"  âš ï¸ ëˆ„ë½ëœ í‚¤: {missing}")
        if unexpected:
            print(f"  âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {unexpected}")

    # 6. í‰ê°€ ëª¨ë“œ ì„¤ì •
    model.to(device)
    model.eval()


    print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (F1: {best_f1})")

    return model, tokenizer


def load_models() -> None:
    """
    ëª¨ë“  ì•™ìƒë¸” ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    FastAPI lifespanì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global _models, _tokenizers

    model_configs = [
        {
            'name': 'kcelectra',
            'checkpoint': os.path.join(settings.MODEL_PATH, 'kcelectra.pt'),
            'hf_model': settings.KCELECTRA_MODEL
        },
        {
            'name': 'soongsil',
            'checkpoint': os.path.join(settings.MODEL_PATH, 'soongsil.pt'),
            'hf_model': settings.SOONGSIL_MODEL
        },
        {
            'name': 'roberta',
            'checkpoint': os.path.join(settings.MODEL_PATH, 'roberta_base.pt'),
            'hf_model': settings.ROBERTA_MODEL
        }
    ]

    print(f"ðŸ”„ ëª¨ë¸ ë¡œë”© ì‹œìž‘ (Device: {get_device()})")

    for config in model_configs:
        if not os.path.exists(config['checkpoint']):
            print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {config['checkpoint']}")
            continue

        model, tokenizer = load_single_model(
            model_name=config['name'],
            checkpoint_path=config['checkpoint'],
            hf_model_name=config['hf_model']
        )
        _models[config['name']] = model
        _tokenizers[config['name']] = tokenizer

    print(f"âœ… ì „ì²´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({len(_models)}ê°œ)")


def get_models() -> Dict[str, MultiLabelClassifier]:
    """ë¡œë“œëœ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
    return _models


def get_tokenizers() -> Dict[str, AutoTokenizer]:
    """ë¡œë“œëœ í† í¬ë‚˜ì´ì € ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
    return _tokenizers


def is_models_loaded() -> bool:
    """ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ (ìµœì†Œ 1ê°œ ì´ìƒ)"""
    return len(_models) >= 1
